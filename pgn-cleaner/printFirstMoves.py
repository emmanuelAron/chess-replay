from pyspark.sql import SparkSession
from pyspark.sql.functions import col, slice

# âœ… 1. Initialiser la session Spark
spark = SparkSession.builder \
    .appName("CheckChessJSONL") \
    .getOrCreate()

# âœ… 2. Chemin du fichier
#path = "C:/Users/emman/Desktop/ironhackData/week7/chess_dataset/games_1990_cleaned_final_cleaned.jsonl"
path = "games_1990_cleaned.jsonl"


print(f"ğŸ“‚ Lecture du fichier : {path}\n")

# âœ… 3. Lecture du JSONL
try:
    df = spark.read.json(path)
except Exception as e:
    print("âŒ Erreur de lecture du JSON :", e)
    spark.stop()
    exit(1)

# âœ… 4. VÃ©rifier si le DataFrame est vide
count = df.count()
if count == 0:
    print("âš ï¸  Le DataFrame est vide ! VÃ©rifie le chemin ou le contenu du fichier.")
    spark.stop()
    exit(0)

print(f"âœ… Fichier chargÃ© avec succÃ¨s ({count} lignes)")

# âœ… 5. Afficher le schÃ©ma dÃ©tectÃ©
print("\nğŸ“˜ SchÃ©ma dÃ©tectÃ© :")
df.printSchema()

# âœ… 6. VÃ©rifier la prÃ©sence des colonnes clÃ©s
expected_cols = {"white", "black", "moves", "date", "event"}
missing = expected_cols - set(df.columns)

if missing:
    print(f"\nâš ï¸ Colonnes manquantes dans le fichier : {missing}")
    print("ğŸ‘‰ VÃ©rifie que le fichier JSONL contient bien ces champs.")
else:
    print("\nâœ… Toutes les colonnes attendues sont prÃ©sentes.")

# âœ… 7. Afficher un aperÃ§u (1 ligne)
print("\nğŸ“„ Exemple d'une ligne :")
df.show(1, truncate=False)

# âœ… 8. Filtrer une partie spÃ©cifique (si la colonne existe)
if "white" in df.columns:
    player_name = "Hernandez Velasco, Jesus"
    print(f"\nğŸ” Recherche des parties jouÃ©es par {player_name}...\n")

    df_filtered = df.filter(col("white") == player_name)

    if df_filtered.count() == 0:
        print(f"âš ï¸ Aucune partie trouvÃ©e pour {player_name}.")
    else:
        df_filtered.select(
            "white", "black", "event", "date",
            slice("moves", 1, 10).alias("first_10_moves")
        ).show(truncate=False)
        # Ecrire premiere partie
        df_filtered.limit(1).coalesce(1).write.mode("overwrite").json("firstGame.json")

else:
    print("\n Impossible de filtrer : la colonne 'white' est absente du fichier.")

spark.stop()

